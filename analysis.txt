Training Loop
    Without Attention:

    100%|██████████| 2171/2171 [02:47<00:00, 12.95it/s]
    Epoch 1: loss=1.9948790073394775, BLEU=0.5939828659509391
    100%|██████████| 2171/2171 [02:49<00:00, 12.83it/s]
    Epoch 2: loss=1.3617374897003174, BLEU=0.6062700613901836
    100%|██████████| 2171/2171 [02:49<00:00, 12.80it/s]
    Epoch 3: loss=1.0753530263900757, BLEU=0.6072836919241708
    100%|██████████| 2171/2171 [02:49<00:00, 12.83it/s]
    Epoch 4: loss=0.8668463826179504, BLEU=0.6070453239905514
    100%|██████████| 2171/2171 [02:49<00:00, 12.82it/s]
    Epoch 5: loss=0.7100824117660522, BLEU=0.6107505372842209
    Finished 5 epochs

    With Attention:

    100%|██████████| 2171/2171 [03:40<00:00,  9.86it/s]
    Epoch 1: loss=1.9084426164627075, BLEU=0.6099366733158523
    100%|██████████| 2171/2171 [03:41<00:00,  9.80it/s]
    Epoch 2: loss=1.2402212619781494, BLEU=0.6300728880949834
    100%|██████████| 2171/2171 [03:41<00:00,  9.78it/s]
    Epoch 3: loss=0.9592753648757935, BLEU=0.6353236444674883
    100%|██████████| 2171/2171 [03:41<00:00,  9.79it/s]
    Epoch 4: loss=0.7581674456596375, BLEU=0.6416396483782354
    100%|██████████| 2171/2171 [03:42<00:00,  9.78it/s]
    Epoch 5: loss=0.6117362976074219, BLEU=0.6375388142258875
    Finished 5 epochs

    With Multi Attention
    NOTE: this is the debugging task as I couldnt get the actual train set to work!
    
    100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 186/186 [00:39<00:00,  4.71it/s]
    Epoch 1: loss=2.207397699356079, BLEU=0.106844945378197
    100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 186/186 [00:40<00:00,  4.57it/s]
    Epoch 2: loss=1.8527048826217651, BLEU=0.2726868713241076
    Finished 2 epochs

Bleu Score
    Without Attention:
    The average BLEU score over the test set was 0.628509917053249

    With Attention:
    The average BLEU score over the test set was 0.6542841967074783

    With Multi Attention

Discussion:

There was a slight descrepancy between training and testing results. 
This may be because the model is underfitting and needs to be trained for more epochs.
The loss is still decresing and has not evidently converged, so we can get better results by running more epochs.

The model without attetion perfomed had a higher loss than the model with attetion, hinting that it was learning the data beter.
THis may be because attention is a mechanism used to store long-term information in sequences. This means that
the attention model is better suited for real scentences and paragraphs, where long-term information is important.
Therefore, the fact that the attention model outperformed the non-attensiton model showed that long-term sequences
data is something that is represented in the data. 
